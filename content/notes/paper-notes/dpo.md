---
Title: Direct Preference Optimization
---




## prelims

RLHF pipelines usually consists of three phases:
1. supervised infe-tuning
2. preference sampling and reward learning
3. reinforcement-learning optimization

SFT phase:
1. begins with generic pre-trained LM
2. fine-tuned with supervised learning with maximum likelihood on a high quality dataset for downstream task ie dialogue, instruction following, summarization
3. result is fine tuned model

Reward Modeling phase:
1. SFT model is prompted with prompts x to produce pairs of answers (y1, y2) ~ SFT( y | x )
2. these are then presented to human labelers who express preferences for one answer denoted as yw > yl | x
3. yw and yl denotes the preferred and dispreferred completions given some prompt
4. the preferences are assumed to be generated by some latent reward model r*(y,x) which we do not have access to
5. some options to model preferences are the Bradley-Terry (BT) model or Plackett-Luce ranking models
6. using the new human annoted dataset of preferred completions, train your preference model

