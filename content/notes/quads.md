---
Title: Quadruples for LLMS
---

### Quadruples for Implementing SOTA Methods
Analogies are one of the strongest features of LLMs.  Both in their ability to teach concepts in terms of analogies relevant to the user, but also in their ability to do 'novel' tasks by inferring what it knows into what the user explains the novel concept to be related to.  With increasing context lengths of high performing models I believe they would be able to help implement SOTA methodology based on paper-code vs paper-*code* examples, where the methods are similar enough to be used as a reference.  Even if the resulting code is not an exact representation of the method in usable code, my belief is that it would at the least be a strong place to start.  

### Quadruples for Evaluating Generalization
Generalization is one of the most important characteristics of machine learning models in general, and significantly more important for the success of modern LLMs.  While a seemingly simple concept to evaluate for, it has been notably difficult to evaluate large models on whether they can generalize or not because their training data often includes so much information it can be hard to determine what is NOT in the training data.  Their ability to recall infromation from training data is one of their strengths while also being a weakness.  It is still valuable to be able to recall any information when your training data includes most of 'everything' but still leaves the question to the ability to provide anything original.  With the same concept from above of using quadruples in prompting LLMS for implement SOTA methods based on research papers, thier ability to output the novel code also is a reference to the models ability to generalize.  The metric in this case would merely be the difference in analogy examples that it is able to implement.  If you provide the LLM a research paper and the respective code implementation, and then provide it a new research paper and ask it to implement the method in code, but the resulting code is merely a 1-2 line difference than the reference code, that it is the extent of its ability.  But if you provide it a paper and code, and then a new paper who's respective code implementation is vastly different than the reference, say out of 100 loc it only shares 50/100 loc from the reference, that difference is its ability to write novel code given a reference.

### Quadruples for Fine Tuning
Triplets have historically been a strong method for training and fine tuning language models.  While it seemed they were losing popularity, a recent case of using triples I found interesting was Google researchers using triplets in knowledge distillation by recording the rationale extracted from large models, and using the task-rationale-result triplets for fine tuning a smaller model to incorprate the learned rationale from the larger model.  I think these types fine tuning examples have lots of promise that has gone unexplored. The one hurdle in this case is being aware of token lengths in examples.  Where performance is often lost in text with lengths greater than 100k, a strong place is in most modern models is the 32k length.  Given quadruples, that leaves 8k tokens for each paper and code example.  Other questions include whether code should include comments or doc strings, or should the model be able to infer these .

### Data Augmentation
To achieve a dataset made up of quadruples, one can use the same quadruple in 4 different training examples, where each example has a different missing section.  